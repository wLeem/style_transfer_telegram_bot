{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "Datasets, that I used to train NN.\n",
    "\n",
    "1. Style images: https://www.kaggle.com/kovalevvyu/painter-by-numbers-resized\n",
    "2. Content images: https://www.kaggle.com/awsaf49/coco-2017-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.models import vgg19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from skimage import io, transform\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", UserWarning)\n",
    "import os\n",
    "import argparse\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.utils import save_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image\n",
    "import cv2\n",
    "from PIL import UnidentifiedImageError\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_mean_std(features):\n",
    "    \"\"\"\n",
    "    :param features: shape of features -> [batch_size, c, h, w]\n",
    "    :return: features_mean, feature_s: shape of mean/std ->[batch_size, c, 1, 1]\n",
    "    \"\"\"\n",
    "\n",
    "    batch_size, c = features.size()[:2]\n",
    "    features_mean = features.reshape(batch_size, c, -1).mean(dim=2).reshape(batch_size, c, 1, 1)\n",
    "    features_std = features.reshape(batch_size, c, -1).std(dim=2).reshape(batch_size, c, 1, 1) + 1e-6\n",
    "    return features_mean, features_std\n",
    "\n",
    "\n",
    "def adain(content_features, style_features):\n",
    "    \"\"\"\n",
    "    Adaptive Instance Normalization\n",
    "    :param content_features: shape -> [batch_size, c, h, w]\n",
    "    :param style_features: shape -> [batch_size, c, h, w]\n",
    "    :return: normalized_features shape -> [batch_size, c, h, w]\n",
    "    \"\"\"\n",
    "    content_mean, content_std = calc_mean_std(content_features)\n",
    "    style_mean, style_std = calc_mean_std(style_features)\n",
    "    normalized_features = style_std * (content_features - content_mean) / content_std + style_mean\n",
    "    return normalized_features\n",
    "\n",
    "\n",
    "class VGGEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        vgg = vgg19(pretrained=True).features\n",
    "        self.slice1 = vgg[: 2]\n",
    "        self.slice2 = vgg[2: 7]\n",
    "        self.slice3 = vgg[7: 12]\n",
    "        self.slice4 = vgg[12: 21]\n",
    "        for p in self.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "    def forward(self, images, output_last_feature=False):\n",
    "        h1 = self.slice1(images)\n",
    "        h2 = self.slice2(h1)\n",
    "        h3 = self.slice3(h2)\n",
    "        h4 = self.slice4(h3)\n",
    "        if output_last_feature:\n",
    "            return h4\n",
    "        else:\n",
    "            return h1, h2, h3, h4\n",
    "\n",
    "\n",
    "class RC(nn.Module):\n",
    "    \"\"\"A wrapper of ReflectionPad2d and Conv2d\"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, pad_size=1, activated=True):\n",
    "        super().__init__()\n",
    "        self.pad = nn.ReflectionPad2d((pad_size, pad_size, pad_size, pad_size))\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)\n",
    "        self.activated = activated\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.pad(x)\n",
    "        h = self.conv(h)\n",
    "        if self.activated:\n",
    "            return F.relu(h)\n",
    "        else:\n",
    "            return h\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.rc1 = RC(512, 256, 3, 1)\n",
    "        self.rc2 = RC(256, 256, 3, 1)\n",
    "        self.rc3 = RC(256, 256, 3, 1)\n",
    "        self.rc4 = RC(256, 256, 3, 1)\n",
    "        self.rc5 = RC(256, 128, 3, 1)\n",
    "        self.rc6 = RC(128, 128, 3, 1)\n",
    "        self.rc7 = RC(128, 64, 3, 1)\n",
    "        self.rc8 = RC(64, 64, 3, 1)\n",
    "        self.rc9 = RC(64, 3, 3, 1, False)\n",
    "\n",
    "    def forward(self, features):\n",
    "        h = self.rc1(features)\n",
    "        h = F.interpolate(h, scale_factor=2)\n",
    "        h = self.rc2(h)\n",
    "        h = self.rc3(h)\n",
    "        h = self.rc4(h)\n",
    "        h = self.rc5(h)\n",
    "        h = F.interpolate(h, scale_factor=2)\n",
    "        h = self.rc6(h)\n",
    "        h = self.rc7(h)\n",
    "        h = F.interpolate(h, scale_factor=2)\n",
    "        h = self.rc8(h)\n",
    "        h = self.rc9(h)\n",
    "        return h\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.vgg_encoder = VGGEncoder()\n",
    "        self.decoder = Decoder()\n",
    "\n",
    "    def generate(self, content_images, style_images, alpha=1.0):\n",
    "        content_features = self.vgg_encoder(content_images, output_last_feature=True)\n",
    "        style_features = self.vgg_encoder(style_images, output_last_feature=True)\n",
    "        t = adain(content_features, style_features)\n",
    "        t = alpha * t + (1 - alpha) * content_features\n",
    "        out = self.decoder(t)\n",
    "        return out\n",
    "\n",
    "    @staticmethod\n",
    "    def calc_content_loss(out_features, t):\n",
    "        return F.mse_loss(out_features, t)\n",
    "\n",
    "    @staticmethod\n",
    "    def calc_style_loss(content_middle_features, style_middle_features):\n",
    "        loss = 0\n",
    "        for c, s in zip(content_middle_features, style_middle_features):\n",
    "            c_mean, c_std = calc_mean_std(c)\n",
    "            s_mean, s_std = calc_mean_std(s)\n",
    "            loss += F.mse_loss(c_mean, s_mean) + F.mse_loss(c_std, s_std)\n",
    "        return loss\n",
    "\n",
    "    def forward(self, content_images, style_images, alpha=1.0, lam=10):\n",
    "        content_features = self.vgg_encoder(content_images, output_last_feature=True)\n",
    "        style_features = self.vgg_encoder(style_images, output_last_feature=True)\n",
    "        t = adain(content_features, style_features)\n",
    "        t = alpha * t + (1 - alpha) * content_features\n",
    "        out = self.decoder(t)\n",
    "\n",
    "        output_features = self.vgg_encoder(out, output_last_feature=True)\n",
    "        output_middle_features = self.vgg_encoder(out, output_last_feature=False)\n",
    "        style_middle_features = self.vgg_encoder(style_images, output_last_feature=False)\n",
    "\n",
    "        loss_c = self.calc_content_loss(output_features, t)\n",
    "        loss_s = self.calc_style_loss(output_middle_features, style_middle_features)\n",
    "        loss = loss_c + lam * loss_s\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "\n",
    "trans = transforms.Compose([transforms.RandomCrop(256),\n",
    "                            transforms.ToTensor(),\n",
    "                            normalize])\n",
    "\n",
    "\n",
    "def denorm(tensor, device):\n",
    "    std = torch.Tensor([0.229, 0.224, 0.225]).reshape(-1, 1, 1).to(device)\n",
    "    mean = torch.Tensor([0.485, 0.456, 0.406]).reshape(-1, 1, 1).to(device)\n",
    "    res = torch.clamp(tensor * std + mean, 0, 1)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_dir = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if make_dir == True:\n",
    "    os.mkdir('/kaggle/working/los_dir/')\n",
    "    os.mkdir('/kaggle/working/model_dir/')\n",
    "    os.mkdir('/kaggle/working/image_dir_epoch/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreprocessDataset(Dataset):\n",
    "    def __init__(self, content_dir, style_dir, transforms=trans):\n",
    "        l1 = content_dir.split('/')[-1]\n",
    "        l2 = style_dir.split('/')[-1]\n",
    "        content_dir_resized = \"/kaggle/working/\" + l1 + '_resized'\n",
    "        style_dir_resized = \"/kaggle/working/\" + l2 + '_resized'\n",
    "        \n",
    "        if not (os.path.exists(content_dir_resized) and\n",
    "                os.path.exists(style_dir_resized)):\n",
    "            os.mkdir(content_dir_resized)\n",
    "            os.mkdir(style_dir_resized)\n",
    "            self._resize(content_dir, content_dir_resized)\n",
    "            self._resize(style_dir, style_dir_resized)\n",
    "        \n",
    "        content_images = glob.glob((content_dir_resized + '/*'))\n",
    "        np.random.shuffle(content_images)\n",
    "        print('content img: ', len(content_images))\n",
    "        style_images = glob.glob(style_dir_resized + '/*')\n",
    "        np.random.shuffle(style_images)\n",
    "        print('style img: ', len(style_images))\n",
    "        self.images_pairs = list(zip(content_images, style_images))\n",
    "        print('pairs: ', len(list(zip(content_images, style_images))))\n",
    "        self.transforms = transforms\n",
    "\n",
    "    @staticmethod\n",
    "    def _resize(source_dir, target_dir):\n",
    "        print(f'Start resizing {source_dir} ')\n",
    "        for i,length in zip(os.listdir(source_dir), range(0,7500)):\n",
    "            filename = os.path.basename(i)\n",
    "            try:\n",
    "                image = io.imread(os.path.join(source_dir, i))\n",
    "                if len(image.shape) == 3 and image.shape[-1] == 3:\n",
    "                    H, W, _ = image.shape\n",
    "                    if H < W:\n",
    "                        ratio = W / H\n",
    "                        H = 512\n",
    "                        W = int(ratio * H)\n",
    "                    else:\n",
    "                        ratio = H / W\n",
    "                        W = 512\n",
    "                        H = int(ratio * W)\n",
    "                    image = transform.resize(image, (H, W), mode='reflect', anti_aliasing=True)\n",
    "                    io.imsave(os.path.join(target_dir, filename), image)\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images_pairs)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        try:\n",
    "            content_image, style_image = self.images_pairs[index]\n",
    "            content_image = Image.open(content_image)\n",
    "            style_image = Image.open(style_image)\n",
    "            if self.transforms:\n",
    "                content_image = self.transforms(content_image)\n",
    "                style_image = self.transforms(style_image)\n",
    "            return content_image, style_image\n",
    "        except UnidentifiedImageError:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    image_dir = '/kaggle/working/image_dir_epoch'\n",
    "    model_dir = '/kaggle/working/model_dir'\n",
    "    \n",
    "    # prepare dataset and dataLoader\n",
    "#     train_dataset = PreprocessDataset('../input/coco-2017-dataset/coco2017/val2017', '../input/claude-monet-pictorial-works-dataset-wikiart')\n",
    "    train_dataset = PreprocessDataset('../input/coco-2017-dataset/coco2017/train2017', '../input/painter-by-numbers-resized')\n",
    "    test_dataset = PreprocessDataset('../input/testing/dataset_test/content', '../input/testing/dataset_test/style')\n",
    "    iters = len(train_dataset)\n",
    "    print(f'Length of train image pairs: {iters}')\n",
    "\n",
    "    batch_size = 32\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_iter = iter(test_loader)\n",
    "\n",
    "    model = Model().to(device)\n",
    "\n",
    "    learning_rate = 5e-5\n",
    "    optimizer = Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    epoch = 30\n",
    "    snapshot_interval = 1000\n",
    "    loss_list = []\n",
    "    for e in range(1, epoch + 1):\n",
    "        print(f'Start {e} epoch')\n",
    "        for i, (content, style) in tqdm(enumerate(train_loader, 1)):\n",
    "            content = content.to(device)\n",
    "            style = style.to(device)\n",
    "            loss = model(content, style)\n",
    "            loss_list.append(loss.item())\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            print(f'[{e}/total {epoch} epoch],[{i} /'\n",
    "                  f'total {round(iters/batch_size)} iteration]: {loss.item()}')\n",
    "\n",
    "            if i % snapshot_interval == 0:\n",
    "                content, style = next(test_iter)\n",
    "                content = content.to(device)\n",
    "                style = style.to(device)\n",
    "                with torch.no_grad():\n",
    "                    out = model.generate(content, style)\n",
    "                content = denorm(content, device)\n",
    "                style = denorm(style, device)\n",
    "                out = denorm(out, device)\n",
    "                res = torch.cat([content, style, out], dim=0)\n",
    "                res = res.to('cpu')\n",
    "                save_image(res, f'{image_dir}/{e}_epoch_{i}_iteration.png', nrow=batch_size)\n",
    "        torch.save(model.state_dict(), f'{model_dir}/{e}_epoch.pth')\n",
    "        print(f'{model_dir}/{e}_epoch.pth')\n",
    "    loss_dir = '/kaggle/working/los_dir'\n",
    "    plt.plot(range(len(loss_list)), loss_list)\n",
    "    plt.xlabel('iteration')\n",
    "    plt.ylabel('loss')\n",
    "    plt.title('train loss')\n",
    "    plt.savefig(f'{loss_dir}/train_loss.png')\n",
    "    with open(f'{loss_dir}/loss_log.txt', 'w') as f:\n",
    "        for l in loss_list:\n",
    "            f.write(f'{l}\\n')\n",
    "    print(f'Loss saved in {loss_dir}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-25T09:12:16.615773Z",
     "iopub.status.busy": "2022-01-25T09:12:16.615147Z",
     "iopub.status.idle": "2022-01-25T09:12:16.62848Z",
     "shell.execute_reply": "2022-01-25T09:12:16.627708Z",
     "shell.execute_reply.started": "2022-01-25T09:12:16.615737Z"
    }
   },
   "outputs": [],
   "source": [
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "\n",
    "trans = transforms.Compose([transforms.ToTensor(),\n",
    "                            normalize])\n",
    "\n",
    "\n",
    "def denorm(tensor, device):\n",
    "    std = torch.Tensor([0.229, 0.224, 0.225]).reshape(-1, 1, 1).to(device)\n",
    "    mean = torch.Tensor([0.485, 0.456, 0.406]).reshape(-1, 1, 1).to(device)\n",
    "    res = torch.clamp(tensor * std + mean, 0, 1)\n",
    "    return res\n",
    "\n",
    "\n",
    "def main_test():\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    model_state_path = './model_dir/20_epoch.pth'\n",
    "    \n",
    "    model = Model()\n",
    "    if model_state_path is not None:\n",
    "        model.load_state_dict(torch.load(model_state_path, map_location=lambda storage, loc: storage))\n",
    "    model = model.to(device)\n",
    "\n",
    "    c = Image.open('../input/testing/dataset_test/content/doggie.jpg')\n",
    "    s = Image.open('../input/painter-by-numbers-resized/0.jpg')\n",
    "    c_tensor = trans(c).unsqueeze(0).to(device)\n",
    "    s_tensor = trans(s).unsqueeze(0).to(device)\n",
    "    alpha = 1\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(c_tensor, s_tensor, alpha)\n",
    "\n",
    "    out = denorm(out, device)\n",
    "\n",
    "    output_name = './test/test_3'\n",
    "    save_image(out, f'{output_name}.jpg', nrow=1)\n",
    "\n",
    "    print(f'result saved into files starting with {output_name}')\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    print(f'inference neural network is {end_time-start_time}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-25T09:12:16.991942Z",
     "iopub.status.busy": "2022-01-25T09:12:16.991711Z",
     "iopub.status.idle": "2022-01-25T09:12:19.010537Z",
     "shell.execute_reply": "2022-01-25T09:12:19.009777Z",
     "shell.execute_reply.started": "2022-01-25T09:12:16.991914Z"
    }
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    main_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-25T09:12:20.121994Z",
     "iopub.status.busy": "2022-01-25T09:12:20.1215Z",
     "iopub.status.idle": "2022-01-25T09:12:20.5594Z",
     "shell.execute_reply": "2022-01-25T09:12:20.558553Z",
     "shell.execute_reply.started": "2022-01-25T09:12:20.121959Z"
    }
   },
   "source": [
    "![Train loss](train_loss.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
